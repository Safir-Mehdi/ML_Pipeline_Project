{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils' from 'f:\\\\data science\\\\ml projects\\\\ml project by engineering wala bhaiya\\\\ml_pipeline_project\\\\src\\\\utils.py'>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import src\n",
    "import joblib\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "from typing import Union\n",
    "from src.utils import fetch_data, FrequencyEncoder, Winsorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, TargetEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "importlib.reload(src.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Setting the basepath\\nos.chdir('f:\\\\Data Science\\\\ML Projects\\\\ML Project by Engineering Wala Bhaiya\\\\ML_Pipeline_Project')\\nBASE_PATH = os.getcwd()\\n\\n# Importing the dataset from the source\\ntry:\\n    RAW_DATA_PATH = os.path.join(BASE_PATH, 'data', 'raw')\\n    income_data = pd.read_csv(os.path.join(RAW_DATA_PATH, os.listdir(RAW_DATA_PATH)[0]))\\nexcept Exception as e:\\n    error = CustomException(error_message=e, error_detail=sys)\\n    logging.info(error.error_message)\\n    raise e\\n\\n# Strpping all columns and values from the object data\\nincome_data.columns = income_data.columns.str.strip()\\ntemp_df = income_data.select_dtypes(include=['object']).apply(lambda x: x.str.strip())\\nincome_data.drop(temp_df.columns, axis=1, inplace=True)\\nincome_data = pd.concat([income_data, temp_df], axis=1)\\n\\ndel(temp_df)\\n\\nincome_data.head()\\n\\n\\n# Visualizing the winsorization of the data\\n\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an old approach to used in data insertion\n",
    "'''# Setting the basepath\n",
    "os.chdir('f:\\\\Data Science\\\\ML Projects\\\\ML Project by Engineering Wala Bhaiya\\\\ML_Pipeline_Project')\n",
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "# Importing the dataset from the source\n",
    "try:\n",
    "    RAW_DATA_PATH = os.path.join(BASE_PATH, 'data', 'raw')\n",
    "    income_data = pd.read_csv(os.path.join(RAW_DATA_PATH, os.listdir(RAW_DATA_PATH)[0]))\n",
    "except Exception as e:\n",
    "    error = CustomException(error_message=e, error_detail=sys)\n",
    "    logging.info(error.error_message)\n",
    "    raise e\n",
    "\n",
    "# Strpping all columns and values from the object data\n",
    "income_data.columns = income_data.columns.str.strip()\n",
    "temp_df = income_data.select_dtypes(include=['object']).apply(lambda x: x.str.strip())\n",
    "income_data.drop(temp_df.columns, axis=1, inplace=True)\n",
    "income_data = pd.concat([income_data, temp_df], axis=1)\n",
    "\n",
    "del(temp_df)\n",
    "\n",
    "income_data.head()\n",
    "\n",
    "\n",
    "# Visualizing the winsorization of the data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print(income_data['hours-per-week'].describe())\n",
    "sns.histplot(income_data['hours-per-week'], bins=30, kde=True, color=\"blue\")\n",
    "plt.axvline(3.0, color='red', linestyle='--', label='Lower Bound (3.0)')\n",
    "plt.axvline(75.0, color='green', linestyle='--', label='Upper Bound (75.0)')\n",
    "plt.title('Distribution of Hours Per Week')\n",
    "plt.xlabel('Hours Per Week')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "import pandas as pd\n",
    "\n",
    "# Winsorize the data\n",
    "winsorized_hours = winsorize(income_data['hours-per-week'], limits=(0.001, 0.012))\n",
    "\n",
    "# Convert the masked array back to a pandas Series for easy handling\n",
    "winsorized_hours = pd.Series(winsorized_hours, index=income_data.index)\n",
    "\n",
    "# Inspect the results\n",
    "print(winsorized_hours.describe())\n",
    "sns.histplot(winsorized_hours, bins=30, kde=True, color=\"purple\")\n",
    "plt.title(\"Distribution After Winsorization\")\n",
    "plt.xlabel(\"Hours Per Week (Winsorized)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Manually clip values to the 0.1% and 98.8% bounds\n",
    "hours_per_week_clipped = income_data['hours-per-week'].clip(lower=3.0, upper=75.0)\n",
    "\n",
    "# Inspect the results\n",
    "print(hours_per_week_clipped.describe())\n",
    "sns.histplot(hours_per_week_clipped, bins=30, kde=True, color=\"green\")\n",
    "plt.title(\"Manual Clipping of Hours Per Week\")\n",
    "plt.xlabel(\"Hours Per Week (Clipped)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "    # This is more compact way to write a same code\n",
    "'''\n",
    "transform_features = [\n",
    "    sublist\n",
    "    for i in range(len(preprocessor.transformers_))\n",
    "    for sublist in preprocessor.transformers_[i][2]\n",
    "]\n",
    "transform_features.extend(target_preprocessor.transformers_[0][2])\n",
    "transform_features\n",
    "\n",
    "# But this not comes into that one\n",
    "transform_features = [\n",
    "    item\n",
    "    for sublist in [\n",
    "        preprocessor.transformers_[i][2]\n",
    "        for i in range(len(preprocessor.transformers_) - 1)\n",
    "        ]\n",
    "    for item in sublist\n",
    "    ]\n",
    "'''\n",
    "# This is the list of random forest classifier\n",
    "'''    random_forest_list = [\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced',\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "    for _ in range(len(target_imputers))\n",
    "]'''\n",
    "\n",
    "# This code is used to calculate percentiles for winsorization (but Maunally)\n",
    "'''# Calculate the 99.4th and 99.5th percentiles\n",
    "lower_bound = income_data['hours-per-week'].quantile(0.001)\n",
    "upper_bound = income_data['hours-per-week'].quantile(0.988)\n",
    "\n",
    "# Extract the dataset within the specified range\n",
    "print(income_data[(income_data['hours-per-week'] >= lower_bound) & (income_data['hours-per-week'] <= upper_bound)].shape,\n",
    "income_data.shape[0] - income_data[(income_data['hours-per-week'] >= lower_bound) & (income_data['hours-per-week'] <= upper_bound)].shape[0]\n",
    ")'''\n",
    "\n",
    "# This code is used to extract transformed featuers\n",
    "'''all_feature_names = []\n",
    "for _, transformer, features in preprocessor.transformers_:\n",
    "    if hasattr(transformer, 'get_feature_names_out'):\n",
    "        feature_names = transformer.get_feature_names_out()\n",
    "        all_feature_names.extend(feature_names)\n",
    "    else:\n",
    "        all_feature_names.extend(features)'''\n",
    "\n",
    "# This code is for transforming features in train and test features seperately\n",
    "'''preprocessor.fit(X_train, y_train)\n",
    "# Set feature names for FrequencyEncoder\n",
    "frequency_encoder = preprocessor.named_transformers_['frequency']\n",
    "for name, feature in zip(['frequency', 'winsorizer'], [frequency_features, winsorize_featues]):\n",
    "    preprocessor.named_transformers_[name].set_feature_names(feature)\n",
    "\n",
    "# transform the data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "print(f'Shape of X_train: {X_train_transformed.shape}', f'Shape of X_test: {X_test_transformed.shape}', sep='\\n')'''\n",
    "\n",
    "# This code is for transforming target varibale in train and test seperately\n",
    "'''target_preprocessor = ColumnTransformer(transformers=[\n",
    "    ('target', OrdinalEncoder(categories=[income_caregory]), ['income'])\n",
    "], remainder='passthrough')\n",
    "target_preprocessor.fit(y_train.to_frame(name='income'))\n",
    "y_train_transformed = target_preprocessor.transform(y_train.to_frame(name='income'))\n",
    "y_test_transformed = target_preprocessor.transform(y_test.to_frame(name='income'))\n",
    "print(f'Shape of y_train: {y_train_transformed.shape}', f'Shape of y_test: {y_test_transformed.shape}', sep='\\n')'''\n",
    "\n",
    "# This code is use for saving the preprocess data\n",
    "'''# We will continue the code from combining train and test and save them into the processed folder\n",
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "try:\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        PROCESSED_PATH = os.path.join(DATA_PATH, 'processed')\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            transfromed_features = pd.concat([transforme_DataFrame(X_train_transformed), transforme_DataFrame(X_test_transformed)], axis=0)\n",
    "            target_income = pd.concat([pd.Series(y_train_transformed.reshape(-1), dtype=np.int64, name='income'), pd.Series(y_test_transformed.reshape(-1), dtype=np.int64, name='income')], axis=0)\n",
    "            transfrom_income_data = pd.concat([transfromed_features, target_income], axis=1)\n",
    "            transfrom_income_data.to_csv(os.path.join(PROCESSED_PATH, 'Transformed_Income_Unimputed.csv'), index=False)\n",
    "        else:\n",
    "            print(f'Path {PROCESSED_PATH} does not exist')\n",
    "    else:\n",
    "        print(f'Directory {DATA_PATH} does not exist')\n",
    "except OSError as e:\n",
    "    print(f'OS error: {e}')'''\n",
    "    \n",
    "# This is code is for creating list imputive_features and target_imputers for random forest\n",
    "'''# Splited features & target variables based on the missing values\n",
    "\n",
    "# Fecthing nulled features\n",
    "imputive_columns = [\n",
    "    imputive_features\n",
    "    for imputive_features, imputive_value in zip(\n",
    "        income_data.isnull().sum().index, income_data.isnull().sum().values\n",
    "        )\n",
    "    if imputive_value != 0\n",
    "    ]\n",
    "\n",
    "# Creating Label Encoders for the target imputers\n",
    "workclass_label_encoder, occupation_label_encoder, native_country_label_encoder = LabelEncoder(), LabelEncoder(), LabelEncoder()\n",
    "encoder_list = [workclass_label_encoder, occupation_label_encoder, native_country_label_encoder]\n",
    "\n",
    "# Creating sets of imputive features & target imputers  based on the nulled values\n",
    "imputive_features = []\n",
    "target_imputers = []\n",
    "for imputive_feature, encoder in zip(imputive_columns, encoder_list):\n",
    "    # Just have appended multiple imputive features according to the target variables\n",
    "    imputive_features.append(\n",
    "        transfrom_income_data.drop(columns=imputive_feature).values\n",
    "    )\n",
    "    \n",
    "    # Applied Label Encoding on multiple targer imputers\n",
    "    transformed_target = pd.concat(\n",
    "        [\n",
    "            pd.Series(encoder.fit_transform(X_train[imputive_feature]), name=imputive_feature),\n",
    "            pd.Series(encoder.fit_transform(X_test[imputive_feature]), name=imputive_feature)\n",
    "        ]\n",
    "    )\n",
    "    target_imputers.append(\n",
    "        transformed_target.values\n",
    "    )\n",
    "    \n",
    "\n",
    "print(\n",
    "    f'Total Set of Imputive Features: {len(imputive_features)}',\n",
    "    f'Total Set of Target Imputers: {len(target_imputers)}',\n",
    "    sep='\\n'\n",
    ")'''\n",
    "\n",
    "# This code is for imputining missing values using Random Forest Imputer\n",
    "'''# Creating the sub-set of imputation_set where native-country has a null values\n",
    "imputation_set_native_country = imputation_set[\n",
    "    imputation_set['native-country'].isna() &\n",
    "    ~imputation_set['workclass'].isna() &\n",
    "    ~imputation_set['occupation'].isna()\n",
    "].copy()\n",
    "\n",
    "\n",
    "# -> Start make this as a function <-\n",
    "# Transforming the sub-set where native-country has a null value\n",
    "X_imputation_set_native_country = preprocessor.transform(\n",
    "    imputation_set_native_country.drop(columns=['income'])\n",
    "    )\n",
    "\n",
    "# Converting them into DataFrame for droping native-country\n",
    "X_imputation_set_native_country = transforme_DataFrame(\n",
    "    X_imputation_set_native_country\n",
    "    ).drop(columns=['native-country'])\n",
    "\n",
    "# then transforming income\n",
    "income_imputation_set_native_country = target_preprocessor.transform(\n",
    "    imputation_set_native_country['income'].to_frame(name='income')\n",
    "    ).ravel()\n",
    "\n",
    "# and  in the last we are concatinating into a single set of features\n",
    "X_imputation_set_native_country = pd.concat(\n",
    "    [\n",
    "        X_imputation_set_native_country,\n",
    "        pd.Series(income_imputation_set_native_country, name='income')\n",
    "        ], axis=1)\n",
    "\n",
    "# Now put that native country subset into random forest for prediction\n",
    "predictive_native_country = random_forest_list['native-country'].predict(X_imputation_set_native_country.values)\n",
    "imputation_set_native_country['predictive_native_country'] = predictive_native_country\n",
    "\n",
    "# And at the end here are the results\n",
    "imputation_set_native_country.head()'''\n",
    "\n",
    "# This code used for comparising t1 with t2\n",
    "'''t1 = income_data_nona['occupation'].unique()\n",
    "t2 = encoder_dict['occupation'].classes_\n",
    "\n",
    "# Find elements in t1 that are not in t2\n",
    "for i in range(len(t1)):\n",
    "    is_present = False\n",
    "    for j in range(len(t2)):\n",
    "        if t1[i] == t2[j]:\n",
    "            is_present = True\n",
    "            break\n",
    "    if not is_present:\n",
    "        print(f\"Value {t1[i]} is not present in t2\")'''\n",
    "\n",
    "# This code is used to take out the NaN values from the imputation_set based on different combunations\n",
    "'''imputation_set[\n",
    "    imputation_set['native-country'].isna() &\n",
    "    imputation_set['workclass'].isna() &\n",
    "    imputation_set['occupation'].isna()\n",
    "    ].shape\n",
    "    \n",
    "imputation_set[\n",
    "    imputation_set['workclass'].isna() &\n",
    "    imputation_set['occupation'].isna() &\n",
    "    ~imputation_set['native-country'].isna()\n",
    "].shape\n",
    "\n",
    "imputation_set[\n",
    "    imputation_set['workclass'].isna() &\n",
    "    ~imputation_set['occupation'].isna() &\n",
    "    ~imputation_set['native-country'].isna()\n",
    "].shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week   \n",
       "0   39   77516             13          2174             0              40  \\\n",
       "1   50   83311             13             0             0              13   \n",
       "2   38  215646              9             0             0              40   \n",
       "3   53  234721              7             0             0              40   \n",
       "4   28  338409             13             0             0              40   \n",
       "\n",
       "          workclass  education      marital-status         occupation   \n",
       "0         State-gov  Bachelors       Never-married       Adm-clerical  \\\n",
       "1  Self-emp-not-inc  Bachelors  Married-civ-spouse    Exec-managerial   \n",
       "2           Private    HS-grad            Divorced  Handlers-cleaners   \n",
       "3           Private       11th  Married-civ-spouse  Handlers-cleaners   \n",
       "4           Private  Bachelors  Married-civ-spouse     Prof-specialty   \n",
       "\n",
       "    relationship   race     sex native-country income  \n",
       "0  Not-in-family  White    Male  United-States  <=50K  \n",
       "1        Husband  White    Male  United-States  <=50K  \n",
       "2  Not-in-family  White    Male  United-States  <=50K  \n",
       "3        Husband  Black    Male  United-States  <=50K  \n",
       "4           Wife  Black  Female           Cuba  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_data = fetch_data(FILE_NAME='income_evaluation.csv', DIRECTORY_NAME='raw')\n",
    "income_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   fnlwgt          32561 non-null  int64 \n",
      " 2   education-num   32561 non-null  int64 \n",
      " 3   capital-gain    32561 non-null  int64 \n",
      " 4   capital-loss    32561 non-null  int64 \n",
      " 5   hours-per-week  32561 non-null  int64 \n",
      " 6   workclass       32561 non-null  object\n",
      " 7   education       32561 non-null  object\n",
      " 8   marital-status  32561 non-null  object\n",
      " 9   occupation      32561 non-null  object\n",
      " 10  relationship    32561 non-null  object\n",
      " 11  race            32561 non-null  object\n",
      " 12  sex             32561 non-null  object\n",
      " 13  native-country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "income_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "fnlwgt               0\n",
       "education-num        0\n",
       "capital-gain         0\n",
       "capital-loss         0\n",
       "hours-per-week       0\n",
       "workclass         1836\n",
       "education            0\n",
       "marital-status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "native-country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_data.replace(to_replace='?', value= np.nan, inplace=True)\n",
    "income_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoding Caregorical Features\n",
    "Here we econde the categorical features with using different encoding techniques that fits best for the each. The encoding techniques are listed following:\n",
    "- One-Hot Encoding\n",
    "- Ordinal Encoding\n",
    "- Frequencing Encoding\n",
    "- Target Encoding\n",
    "<br><br>\n",
    "***(Note) - Here we have tested each encoding technique, but for implementing these techniques we have to `encode` each encoder on `training set` to avoid `data leakage`.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass          8\n",
       "education         16\n",
       "marital-status     7\n",
       "occupation        14\n",
       "relationship       6\n",
       "race               5\n",
       "sex                2\n",
       "native-country    41\n",
       "income             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_data.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. One Hot Encoding:\n",
    "We use `One-Hot Encoding` to convert categorical features into numerical features. This technique is suitable for features that do not have a natural order.\n",
    "<br><br>\n",
    "**We will apply this technique on the following features**\n",
    "- sex\n",
    "- race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>race_Asian-Pac-Islander</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex_Male  race_Asian-Pac-Islander  race_Black  race_Other  race_White\n",
       "0       1.0                      0.0         0.0         0.0         1.0\n",
       "1       1.0                      0.0         0.0         0.0         1.0\n",
       "2       1.0                      0.0         0.0         0.0         1.0\n",
       "3       1.0                      0.0         1.0         0.0         0.0\n",
       "4       0.0                      0.0         1.0         0.0         0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making One Hot Encoder & Selecting features\n",
    "one_hot_encoder = OneHotEncoder(drop='first')\n",
    "selected_features = income_data[['sex', 'race']]\n",
    "\n",
    "# Fit and transforming the selected features\n",
    "one_hot_features = one_hot_encoder.fit_transform(selected_features)\n",
    "one_hot_features = one_hot_features.toarray()\n",
    "\n",
    "# Adding the one-hot encoded features to the DataFrame\n",
    "one_hot_feature_names = [\n",
    "    f'{feature}_{category}'\n",
    "    for feature, categories in zip(selected_features.columns, one_hot_encoder.categories_)\n",
    "    for category in categories[1:]\n",
    "    ]\n",
    "one_hot_features = pd.DataFrame(one_hot_features, columns=one_hot_feature_names)\n",
    "one_hot_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Binary Encoding:\n",
    "Now we implementing `Binary Encoding` to convert our target varible into numerical form.\n",
    "<br><br>\n",
    "**We will apply this technique on the following Target feature**\n",
    "- income\n",
    "\n",
    "<br><br><br>\n",
    "***(Note) - Instead of continuing with `Binary Encoding` we use Oridinal Encoding to encode `target variable`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_0</th>\n",
       "      <th>income_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income_0  income_1\n",
       "0         0         1\n",
       "1         0         1\n",
       "2         0         1\n",
       "3         0         1\n",
       "4         0         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by understanding that how this mapping works\n",
    "target_binary_encoder = ce.BinaryEncoder(cols=['income'], verbose=True)\n",
    "target_binary_feature = target_binary_encoder.fit_transform(income_data['income'])\n",
    "target_binary_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ordinal Encoding:\n",
    "`Ordinal encoding` is a technique used to convert categorical variables into numerical variables. It assigns a numerical value to each category based on its order or ranking.<br><br>\n",
    "**We will apply this technique on the following feature**\n",
    "- income (Target variable)\n",
    "- education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income  education\n",
       "0     0.0       12.0\n",
       "1     0.0       12.0\n",
       "2     0.0        8.0\n",
       "3     0.0        6.0\n",
       "4     0.0       12.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordering categories according to the features\n",
    "education_category = [\n",
    "    'Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th',\n",
    "    'HS-grad', 'Some-college', 'Assoc-acdm', 'Assoc-voc', 'Bachelors', 'Masters',\n",
    "    'Prof-school', 'Doctorate'\n",
    "    ]\n",
    "income_caregory = ['<=50K', '>50K']\n",
    "\n",
    "# Creating Ordinal Encoder to encode features, tramsforming & converting then into DataFrame\n",
    "ordinal_encoder = OrdinalEncoder(categories=[income_caregory, education_category])\n",
    "ordinal_feature = ordinal_encoder.fit_transform(income_data[['income', 'education']])\n",
    "# target_ordinal_feature.reshape(-1)\n",
    "ordinal_feature\n",
    "ordinal_feature = pd.DataFrame(ordinal_feature, columns=['income', 'education'])\n",
    "ordinal_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Frequency Encoding:\n",
    "`Frequency encoding` is a technique used in machine learning and data analysis to transform categorical data into numerical data based on the frequency of each category. This technique is useful when dealing with categorical data that has no inherent order or ranking.<br><br>\n",
    "**We will apply this technique for the following fueatures**\n",
    "- workclass\n",
    "- occupation\n",
    "- native-country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worlclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.115783</td>\n",
       "      <td>0.895857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078038</td>\n",
       "      <td>0.124873</td>\n",
       "      <td>0.895857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.697030</td>\n",
       "      <td>0.042075</td>\n",
       "      <td>0.895857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.697030</td>\n",
       "      <td>0.042075</td>\n",
       "      <td>0.895857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.697030</td>\n",
       "      <td>0.127146</td>\n",
       "      <td>0.002918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worlclass  occupation  native-country\n",
       "0   0.039864    0.115783        0.895857\n",
       "1   0.078038    0.124873        0.895857\n",
       "2   0.697030    0.042075        0.895857\n",
       "3   0.697030    0.042075        0.895857\n",
       "4   0.697030    0.127146        0.002918"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the frequency of each variable\n",
    "workclass_frequency = income_data['workclass'].value_counts()/len(income_data['workclass'])\n",
    "occupation_frequency = income_data['occupation'].value_counts()/len(income_data['occupation'])\n",
    "native_country_frequency = income_data['native-country'].value_counts()/len(income_data['native-country'])\n",
    "\n",
    "# And then mapping the calculated frequency on each variable & converting them into DataFrame\n",
    "workclass_frequency = income_data['workclass'].map(workclass_frequency)\n",
    "occupation_frequency = income_data['occupation'].map(occupation_frequency)\n",
    "native_country_frequency = income_data['native-country'].map(native_country_frequency)\n",
    "frequency_features = pd.DataFrame({\n",
    "    'worlclass': workclass_frequency,\n",
    "    'occupation': occupation_frequency,\n",
    "    'native-country': native_country_frequency})\n",
    "frequency_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worlclass         1836\n",
       "occupation        1843\n",
       "native-country     583\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequency_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Target Encoding:\n",
    "We use `Target Encoder` to encode the target variable. This is a technique used to handle imbalanced data by encoding the target & it's also used when the categorical variable are highly coorelated with the target variable.<br><br>\n",
    "**We apply this technique for the following features**\n",
    "- relationship\n",
    "- marital-status\n",
    "<br><br><br>\n",
    "<b><u><p>(Note) - `Target Encoding` contain data leakeg with in the training set if apply validation split, so we have to implement this technique while model validation</p></u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relationship</th>\n",
       "      <th>marital-status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.103070</td>\n",
       "      <td>0.045961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.448571</td>\n",
       "      <td>0.446848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.103070</td>\n",
       "      <td>0.104209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.448571</td>\n",
       "      <td>0.446848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.475128</td>\n",
       "      <td>0.446848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relationship  marital-status\n",
       "0      0.103070        0.045961\n",
       "1      0.448571        0.446848\n",
       "2      0.103070        0.104209\n",
       "3      0.448571        0.446848\n",
       "4      0.475128        0.446848"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the TargetEncoder\n",
    "target_encoder = ce.TargetEncoder(cols=['relationship', 'marital-status'])\n",
    "\n",
    "# Fit the encoder on the training data (target column: 'income')\n",
    "target_encoder.fit(income_data[['relationship', 'marital-status']], ordinal_feature['income'])\n",
    "\n",
    "# Transform the categorical columns and add them to the dataset\n",
    "target_encoder_features = target_encoder.transform(income_data[['relationship', 'marital-status']])\n",
    "target_encoder_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houre-per-week Upper Percentile: 75.0\n",
      "houre-per-week Lower Percentile: 3.0\n",
      "capital-loss Upper Percentile: 2174.0\n",
      "capital-loss Lower Percentile: 0.0\n",
      "capital-gain Upper Percentile: 20051.0\n",
      "capital-gain Lower Percentile: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"houre-per-week Upper Percentile: {income_data['hours-per-week'].quantile(0.988)}\")\n",
    "print(f\"houre-per-week Lower Percentile: {income_data['hours-per-week'].quantile(0.001)}\")\n",
    "print(f\"capital-loss Upper Percentile: {income_data['capital-loss'].quantile(0.993)}\")\n",
    "print(f\"capital-loss Lower Percentile: {income_data['capital-loss'].quantile(0.0)}\")\n",
    "print(f\"capital-gain Upper Percentile: {income_data['capital-gain'].quantile(0.993)}\")\n",
    "print(f\"capital-gain Lower Percentile: {income_data['capital-gain'].quantile(0.05)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 13 40 ... 40 20 40]\n",
      "[ 2174     0     0 ...     0     0 15024]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    winsorize(income_data['hours-per-week'].values, limits=(0.001, 0.012)).data,\n",
    "    winsorize(income_data['capital-gain'].values, limits=(0.05, 0.007)).data,\n",
    "    winsorize(income_data['capital-loss'].values, limits=(0.00, 0.007)).data,\n",
    "    sep='\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Transformation\n",
    "There we will be using SK-Learn ColumnTransformer to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the orignal income set: (32561, 15)\n",
      "Size of missing set: (2399, 15)\n",
      "Size of the non-missing set: (30162, 15)\n",
      "Total size of the missing and non-missing set: (32561, 15)\n"
     ]
    }
   ],
   "source": [
    "# Sates of the missing and non-missing set\n",
    "print(f'Size of the orignal income set: {income_data.shape}')\n",
    "print(f'Size of missing set: {income_data[income_data.isna().any(axis=1)].shape}')\n",
    "print(f'Size of the non-missing set: {income_data.dropna().shape}')\n",
    "print(f'Total size of the missing and non-missing set: ({income_data[income_data.isna().any(axis=1)].shape[0] + income_data.dropna().shape[0]}, {income_data.dropna().shape[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features:  (25637, 14), Shape of training labels: (25637,)\n",
      "Shape of testing features:  (4525, 14), Shape of testing labels: (4525,)\n",
      "Total training and test smaple: (30162, 14)\n"
     ]
    }
   ],
   "source": [
    "# Spliting income set based on the missing values for imputation\n",
    "imputation_set = income_data[income_data.isna().any(axis=1)].copy()\n",
    "income_data_nona = income_data.dropna().copy()\n",
    "\n",
    "# Split the income data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(income_data_nona.drop(columns=['income']), income_data_nona['income'], test_size=0.15, random_state=42)\n",
    "print(f'Shape of training features:  {X_train.shape}, Shape of training labels: {y_train.shape}')\n",
    "print(f'Shape of testing features:  {X_test.shape}, Shape of testing labels: {y_test.shape}')\n",
    "print(f'Total training and test smaple: ({X_train.shape[0] + X_test.shape[0]}, {X_train.shape[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for each encoding technique\n",
    "onehot_features = ['sex', 'race']\n",
    "ordinal_features = ['education']\n",
    "frequency_features = ['workclass', 'occupation', 'native-country']\n",
    "target_features = ['relationship', 'marital-status']\n",
    "winsorize_featues = ['hours-per-week', 'capital-gain', 'capital-loss']\n",
    "limits = [(0.001, 0.988), (0.05, 0.993), (0.00, 0.993)]\n",
    "feature_limits = {feature: limit for feature, limit in zip(winsorize_featues, limits)}\n",
    "income_caregory = ['<=50K', '>50K']\n",
    "remaining_features = income_data.columns[:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(drop='first'), onehot_features),\n",
    "        ('ordinal', OrdinalEncoder(), ordinal_features),\n",
    "        ('frequency', FrequencyEncoder(), frequency_features),\n",
    "        ('target', TargetEncoder(), target_features),\n",
    "        ('winsorizer', Winsorizer(feature_limits=feature_limits), winsorize_featues),\n",
    "        ('remainders', 'passthrough', remaining_features),\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns as-is (numetical features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features in Income Dataset: (30162, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train = income_data_nona.drop(columns=['income'])\n",
    "y_train = income_data_nona['income']\n",
    "preprocessor.fit(X_train, y_train)\n",
    "# Set feature names for FrequencyEncoder\n",
    "# frequency_encoder = preprocessor.named_transformers_['frequency']\n",
    "for name, feature in zip(['frequency', 'winsorizer'], [frequency_features, winsorize_featues]):\n",
    "    preprocessor.named_transformers_[name].set_feature_names(feature)\n",
    "\n",
    "# transform the data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "# X_test_transformed = preprocessor.transform(X_test)\n",
    "print(f'Shape of the Features in Income Dataset: {X_train_transformed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforme_DataFrame(X_transformed):\n",
    "    all_feature_names = []\n",
    "    for _, transformer, features in preprocessor.transformers_:\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            feature_names = transformer.get_feature_names_out()\n",
    "            all_feature_names.extend(feature_names)\n",
    "        else:\n",
    "            all_feature_names.extend(features)\n",
    "    exclude_features = len(features)\n",
    "\n",
    "    # X_train_transformed[:, :-temp].shape, len(all_feature_names)\n",
    "    # return pd.DataFrame(X_transformed[:, :-exclude_features], columns=all_feature_names[:-exclude_features])\n",
    "    return pd.DataFrame(X_transformed, columns=all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Target in Income Dataset: (30162, 1)\n"
     ]
    }
   ],
   "source": [
    "# This Cell is For Experimentation\n",
    "y_train = income_data_nona['income']\n",
    "target_preprocessor = ColumnTransformer(transformers=[\n",
    "    ('target', OrdinalEncoder(categories=[income_caregory]), ['income'])\n",
    "], remainder='passthrough')\n",
    "target_preprocessor.fit(y_train.to_frame(name='income'))\n",
    "y_train_transformed = target_preprocessor.transform(y_train.to_frame(name='income'))\n",
    "# y_test_transformed = target_preprocessor.transform(y_test.to_frame(name='income'))\n",
    "print(f'Shape of Target in Income Dataset: {y_train_transformed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train_transformed.reshape(-1), dtype=np.int64, name='income').to_frame('income').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Processed Data\n",
    "There we have `saved unimputed data`, that gonna use later in the Random Forest to learning imputation\n",
    "<br><br><br>\n",
    "<b><u><i>Note - Please do not run the following cell again. If you don't the file then you could</i></u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell is For Experimentation\n",
    "\n",
    "# We will continue the code from combining train and test and save them into the processed folder\n",
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "try:\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        PROCESSED_PATH = os.path.join(DATA_PATH, 'processed')\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            transfrom_income_data = pd.concat(\n",
    "                [\n",
    "                    transforme_DataFrame(X_train_transformed),\n",
    "                    pd.Series(y_train_transformed.ravel(), name='income')],\n",
    "                axis=1)\n",
    "            transfrom_income_data.to_csv(\n",
    "                os.path.join(PROCESSED_PATH,'Transformed_Income_Unimputed_Without_NaN.csv'),\n",
    "                index=False)\n",
    "        else:\n",
    "            print(f'Path {PROCESSED_PATH} does not exist')\n",
    "    else:\n",
    "        print(f'Directory {DATA_PATH} does not exist')\n",
    "except OSError as e:\n",
    "    print(f'OS error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data Shape: (30162, 18)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Transformed Data Shape: {transfrom_income_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hendling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imputetaion with Random Forest\n",
    "While will use `Random Forest` to impute missing values, and which is also a populer approach handle missing values, The idea is to `Train Random Forest` on non missing data, and to use `model to impute` the data that contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fetch Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>race_Asian-Pac-Islander</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_White</th>\n",
       "      <th>education</th>\n",
       "      <th>workclass</th>\n",
       "      <th>occupation</th>\n",
       "      <th>native-country</th>\n",
       "      <th>relationship</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.042404</td>\n",
       "      <td>0.123367</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.048329</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.082853</td>\n",
       "      <td>0.132352</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.454940</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.738877</td>\n",
       "      <td>0.044758</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.107279</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738877</td>\n",
       "      <td>0.044758</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.454940</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.738877</td>\n",
       "      <td>0.133877</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.493366</td>\n",
       "      <td>0.454940</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex_Male  race_Asian-Pac-Islander  race_Black  race_Other  race_White   \n",
       "0       1.0                      0.0         0.0         0.0         1.0  \\\n",
       "1       1.0                      0.0         0.0         0.0         1.0   \n",
       "2       1.0                      0.0         0.0         0.0         1.0   \n",
       "3       1.0                      0.0         1.0         0.0         0.0   \n",
       "4       0.0                      0.0         1.0         0.0         0.0   \n",
       "\n",
       "   education  workclass  occupation  native-country  relationship   \n",
       "0        9.0   0.042404    0.123367        0.911876      0.106533  \\\n",
       "1        9.0   0.082853    0.132352        0.911876      0.455647   \n",
       "2       11.0   0.738877    0.044758        0.911876      0.106533   \n",
       "3        1.0   0.738877    0.044758        0.911876      0.455647   \n",
       "4        9.0   0.738877    0.133877        0.003050      0.493366   \n",
       "\n",
       "   marital-status  hours-per-week  capital-gain  capital-loss   age    fnlwgt   \n",
       "0        0.048329            40.0        2174.0           0.0  39.0   77516.0  \\\n",
       "1        0.454940            13.0           0.0           0.0  50.0   83311.0   \n",
       "2        0.107279            40.0           0.0           0.0  38.0  215646.0   \n",
       "3        0.454940            40.0           0.0           0.0  53.0  234721.0   \n",
       "4        0.454940            40.0           0.0           0.0  28.0  338409.0   \n",
       "\n",
       "   education-num  income  \n",
       "0           13.0     0.0  \n",
       "1           13.0     0.0  \n",
       "2            9.0     0.0  \n",
       "3            7.0     0.0  \n",
       "4           13.0     0.0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfrom_income_data = fetch_data(FILE_NAME='Transformed_Income_Unimputed_Without_NaN.csv', DIRECTORY_NAME='processed')\n",
    "transfrom_income_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Created a Set of Features and Target Varibale\n",
    "There we are creating set of features and target varibale in a form of list, where in the list each feature set corrispond to each target varibale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Set of Imputive Features: 3\n",
      "Total Set of Target Imputers: 3\n"
     ]
    }
   ],
   "source": [
    "# Splited features & target variables based on the missing values\n",
    "\n",
    "# Fecthing nulled features\n",
    "imputive_columns = [\n",
    "    imputive_features\n",
    "    for imputive_features, imputive_value in zip(\n",
    "        income_data.isnull().sum().index,\n",
    "        income_data.isnull().sum().values\n",
    "        )\n",
    "    if imputive_value != 0\n",
    "    ]\n",
    "\n",
    "# Creating Label Encoders for the target imputers\n",
    "workclass_label_encoder, occupation_label_encoder, native_country_label_encoder = LabelEncoder(), LabelEncoder(), LabelEncoder()\n",
    "encoder_dict = {'workclass': workclass_label_encoder, 'occupation': occupation_label_encoder, 'native_country': native_country_label_encoder}\n",
    "\n",
    "# Creating sets of imputive features & target imputers  based on the nulled values\n",
    "imputive_features = []\n",
    "target_imputers = []\n",
    "for imputive_feature, encoder_key in zip(imputive_columns, encoder_dict.keys()):\n",
    "    \n",
    "    # Just have appended multiple imputive features according to the target variables\n",
    "    imputive_features.append(\n",
    "        transfrom_income_data.drop(columns=imputive_feature).values\n",
    "    )\n",
    "    \n",
    "    # Applied Label Encoding on multiple targer imputers\n",
    "    target_imputers.append(\n",
    "        encoder_dict[encoder_key].fit_transform(income_data_nona[imputive_feature])\n",
    "    )\n",
    "    \n",
    "\n",
    "print(\n",
    "    f'Total Set of Imputive Features: {len(imputive_features)}',\n",
    "    f'Total Set of Target Imputers: {len(target_imputers)}',\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30162, 17), (30162,))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputive_features[0].shape, target_imputers[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make a Function that Impute Each Targer Variable in a Set\n",
    "This `imputation_random_forest` takes different set of features and target variables and then impute each target variable using random forest imputation. It returns a dictionary with the imputed target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_random_forest(\n",
    "    features_list: Union[list, np.ndarray],\n",
    "    targets_list: Union[list, np.ndarray],\n",
    "    imputive_columns: Union[str, list]) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits a list of Random Forest models to impute target values.\n",
    "    \n",
    "    Parameters:\n",
    "    - features_list: list of numpy arrays or pandas DataFrames (features for each target variable)\n",
    "    - targets_list: list of numpy arrays or pandas Series (target variables to impute)\n",
    "\n",
    "    Returns:\n",
    "    - random_forest_dict: dictionary of trained Random Forest models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating empty list to store the random forest models\n",
    "    random_forest_dict = {}\n",
    "    \n",
    "    # Check the validation of all the input parameters - In term of lenght\n",
    "    if not (len(features_list) == len(targets_list) == len(imputive_columns)):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "    \n",
    "    # Train multiple Random Forest models\n",
    "    for index, (X_train, y_train, imputive_variable) in enumerate(\n",
    "        zip(features_list, targets_list, imputive_columns)\n",
    "        ):\n",
    "        # Creating the random forest model\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',\n",
    "            max_features='sqrt',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fitting the model to the training data\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        # Appending the model to the list\n",
    "        random_forest_dict[imputive_variable] = random_forest\n",
    "    return random_forest_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the model\n",
    "Now we have all the components, we can train our `Random Forst` based on the list of features that corrispond to the list of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "random_forest_dict = impute_random_forest(imputive_features, target_imputers, imputive_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Impute Missing Values\n",
    "Then we impute missing values by using Random Forest that we have train before. and we will create combunation of feature sets that don't have target feature.\n",
    "+ We seperate features that have to `impute single value` in a row\n",
    "+ We seperate features that have to `impute multiple values` in a row\n",
    "+ We use Random Forest to `impute missing values` in each feature set\n",
    "+ Then we combine all imputed DataFrames in one DataFrame name `imputation_set`.\n",
    "+ Finally we combine `imputation_set` (contain imputed values in the features insted of missing one) with the `income_data_nona` (contain no missing values) and we get `imputed_income_data` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_target_variable(X_imputive_set: pd.DataFrame, target_feature: str) -> pd.DataFrame:\n",
    "    # Transforming all the features including null and not null values\n",
    "    X_imputation_set = preprocessor.transform(\n",
    "        X_imputive_set.drop(columns=['income'])\n",
    "        )\n",
    "\n",
    "    # Converting them into DataFrame to drop target variable\n",
    "    X_imputation_set = transforme_DataFrame(\n",
    "        X_imputation_set\n",
    "        ).drop(columns=[target_feature])\n",
    "\n",
    "    # then transforming income\n",
    "    income_imputation_set_native_country = target_preprocessor.transform(\n",
    "        X_imputive_set['income'].to_frame(name='income')\n",
    "        ).ravel()\n",
    "\n",
    "    # and  in the last we are concatinating into a single set of features\n",
    "    X_imputation_set = pd.concat(\n",
    "        [\n",
    "            X_imputation_set,\n",
    "            pd.Series(income_imputation_set_native_country, name='income')\n",
    "            ], axis=1\n",
    "        )\n",
    "\n",
    "    # Now put that target variable into random forest for prediction\n",
    "    predictive_feature = random_forest_list[target_feature].predict(X_imputation_set.values)\n",
    "    X_imputive_set[str('predictive_' + target_feature)] = predictive_feature\n",
    "    X_imputive_set = X_imputive_set[str('predictive_' + target_feature)].to_frame(name=str('predictive_' + target_feature))\n",
    "    return X_imputive_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_multi_target(X_imputive_set: pd.DataFrame, target_variables: Union[list, np.array]) -> pd.DataFrame:\n",
    "    X_imputed_list = []\n",
    "    for target in target_variables:\n",
    "        X_imputed = impute_target_variable(X_imputive_set=X_imputive_set, target_feature=target)\n",
    "        X_imputed_list.append(X_imputed)\n",
    "    # Concatenate the list of DataFrames\n",
    "    X_imputed_concat = pd.concat(X_imputed_list, axis=1)\n",
    "    # Drop duplicate columns if any\n",
    "    X_imputive_set = X_imputed_concat.loc[:,~X_imputed_concat.columns.duplicated()]\n",
    "    return X_imputive_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictive_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>Farming-fishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10845</th>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14772</th>\n",
       "      <td>Other-service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23232</th>\n",
       "      <td>Protective-serv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32304</th>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32314</th>\n",
       "      <td>Farming-fishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predictive_occupation\n",
       "5361        Farming-fishing\n",
       "10845          Adm-clerical\n",
       "14772         Other-service\n",
       "20337          Adm-clerical\n",
       "23232       Protective-serv\n",
       "32304          Adm-clerical\n",
       "32314       Farming-fishing"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_set_occupation = imputation_set[\n",
    "    imputation_set['occupation'].isna() &\n",
    "    ~imputation_set['workclass'].isna() &\n",
    "    ~imputation_set['native-country'].isna()\n",
    "].copy()\n",
    "imputation_set_occupation = impute_target_variable(imputation_set_occupation, 'occupation')\n",
    "imputation_set_occupation['predictive_occupation'] = encoder_dict['occupation'].inverse_transform(imputation_set_occupation['predictive_occupation'])\n",
    "imputation_set_occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Features that have to impute single value in a row</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictive_native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Philippines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predictive_native-country\n",
       "14                Philippines\n",
       "38              United-States\n",
       "51              United-States\n",
       "93                    Vietnam\n",
       "245             United-States"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will start the code by testing function -> impute_target_variable\n",
    "imputation_set_native_country = imputation_set[\n",
    "    imputation_set['native-country'].isna() &\n",
    "    ~imputation_set['workclass'].isna() &\n",
    "    ~imputation_set['occupation'].isna()\n",
    "].copy()\n",
    "imputation_set_native_country = impute_target_variable(imputation_set_native_country, 'native-country')\n",
    "# imputation_set_native_country\n",
    "imputation_set_native_country['predictive_native-country'] = encoder_dict['native_country'].inverse_transform(imputation_set_native_country['predictive_native-country'])\n",
    "imputation_set_native_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Features that have to impute multiple value in a row</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictive_workclass</th>\n",
       "      <th>predictive_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Private</td>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Private</td>\n",
       "      <td>Craft-repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Private</td>\n",
       "      <td>Other-service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>State-gov</td>\n",
       "      <td>Adm-clerical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predictive_workclass predictive_occupation\n",
       "27               Private          Adm-clerical\n",
       "69           Federal-gov          Adm-clerical\n",
       "77               Private          Craft-repair\n",
       "106              Private         Other-service\n",
       "128            State-gov          Adm-clerical"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_set_workclass_occupation = imputation_set[\n",
    "    imputation_set['workclass'].isna() &\n",
    "    imputation_set['occupation'].isna() &\n",
    "    ~imputation_set['native-country'].isna()\n",
    "].copy()\n",
    "imputation_set_workclass_occupation = impute_multi_target(imputation_set_workclass_occupation, ['workclass', 'occupation'])\n",
    "# imputation_set_workclass_occupation\n",
    "for f, col in zip(imputation_set_workclass_occupation.columns, ['workclass', 'occupation']):\n",
    "    imputation_set_workclass_occupation[f] = encoder_dict[col].inverse_transform(imputation_set_workclass_occupation[f])\n",
    "imputation_set_workclass_occupation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictive_workclass</th>\n",
       "      <th>predictive_occupation</th>\n",
       "      <th>predictive_native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Private</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Private</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Private</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Private</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>Private</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictive_workclass predictive_occupation predictive_native-country\n",
       "61                Private          Craft-repair                    Mexico\n",
       "297               Private        Prof-specialty             United-States\n",
       "1152              Private     Handlers-cleaners             United-States\n",
       "1676              Private          Tech-support             United-States\n",
       "2513              Private          Adm-clerical             United-States"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_set_workclass_occupation_country = imputation_set[\n",
    "    imputation_set['native-country'].isna() &\n",
    "    imputation_set['workclass'].isna() &\n",
    "    imputation_set['occupation'].isna()\n",
    "    ].copy()\n",
    "imputation_set_workclass_occupation_country = impute_multi_target(\n",
    "    imputation_set_workclass_occupation_country,\n",
    "    ['workclass', 'occupation', 'native-country']\n",
    "    )\n",
    "for f, col in zip(imputation_set_workclass_occupation_country.columns, ['workclass', 'occupation', 'native_country']):\n",
    "    imputation_set_workclass_occupation_country[f] = encoder_dict[col].inverse_transform(imputation_set_workclass_occupation_country[f])\n",
    "imputation_set_workclass_occupation_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_predictors_with_imputation_set(imputation_set, predictor_set, features: list):\n",
    "    existing_values = imputation_set.loc[predictor_set.index, features].notna().any().any()\n",
    "    if existing_values:\n",
    "        raise ValueError(\n",
    "            \"Assignment halted: One or more target features in imputation_set already have non-NaN values.\"\n",
    "        )\n",
    "    predictive_features = ['predictive_' + feature for feature in features]\n",
    "    imputation_set.loc[predictor_set.index, features] = predictor_set[predictive_features].values\n",
    "    return imputation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note - Please ensure that you run the following just a once only***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "fnlwgt            0\n",
       "education-num     0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "workclass         0\n",
       "education         0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "native-country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    predictive_imputive_set_list = [\n",
    "        imputation_set_occupation,\n",
    "        imputation_set_native_country,\n",
    "        imputation_set_workclass_occupation,\n",
    "        imputation_set_workclass_occupation_country\n",
    "        ]\n",
    "    predictive_imputive_features_list = [\n",
    "        ['occupation'],\n",
    "        ['native-country'],\n",
    "        ['workclass', 'occupation'],\n",
    "        ['workclass', 'occupation', 'native-country']\n",
    "    ]\n",
    "    for predictor_set, col in zip(predictive_imputive_set_list,predictive_imputive_features_list):\n",
    "        imputation_set = set_predictors_with_imputation_set(imputation_set, predictor_set, col)\n",
    "except ValueError as e:\n",
    "    error = CustomException(e,sys)\n",
    "    logging.info(error.error_message)\n",
    "imputation_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_income_data = pd.concat([income_data_nona, imputation_set], axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Save Imputed Dataset & Imputer Model\n",
    "At the last we save imputed income data as a CSV formate into data/processed directory & Dictionay of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Save Dataset\n",
    "We have save a dataset in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_income_data.to_csv(\n",
    "    'F:\\Data Science\\ML Projects\\ML Project by Engineering Wala Bhaiya\\ML_Pipeline_Project\\data\\processed\\Imputed_Income_Dataset_RF.csv',\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Save Models\n",
    "We have saved the model in the form of dictionary using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/Random_Forest_Imputuer_Dict.pkl', 'wb') as file:\n",
    "    joblib.dump(random_forest_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
